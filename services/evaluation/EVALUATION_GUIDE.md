# Multimodal Translation Evaluation Guide

## Overview

This evaluation system assesses the quality of multimodal translation outputs generated by **NLLB** (No Language Left Behind) for text translation and **XTTS** (Cross-lingual Text-to-Speech) for audio generation. The pipeline evaluates translations across four African languages: Efik, Igbo, Swahili, and Xhosa.

## Complete System Diagram: Translation Pipeline + Evaluation Metrics

```
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃                          DATA INPUTS & MODELS                                        ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

                     ┌─────────────────────────────────────────┐
                     │   Source Text (English)                 │
                     │   "I saw two names, sent to me by..."   │
                     └──────────────┬──────────────────────────┘
                                    │
                ┌───────────────────┴───────────────────┐
                │                                       │
                ▼                                       ▼
    ┌───────────────────────┐            ┌──────────────────────────┐
    │  Whisper ASR Model    │            │    NLLB Translation      │
    │  (Speech Recognition) │            │    (Text-to-Text)        │
    └───────┬───────────────┘            └────────┬─────────────────┘
            │                                     │
            ▼                                     ▼
    ┌──────────────────┐              ┌─────────────────────────────┐
    │ Source Audio     │              │ Predicted Target Text       │
    │ (English, 16kHz) │              │ (Xhosa)                     │
    └──────────────────┘              │ "Ndabona amagama amabini,   │
            │                         │  athunyelwa kum..."         │
            │                         └──────────┬──────────────────┘
            │                                    │
            │                                    ▼
            │                         ┌─────────────────────────────┐
            │                         │    XTTS TTS Model           │
            │                         │    (Text-to-Speech)         │
            │                         └──────────┬──────────────────┘
            │                                    │
            │                                    ▼
            │                         ┌─────────────────────────────┐
            │                         │ Predicted Target Audio      │
            │                         │ (Xhosa, 16kHz)              │
            │                         └─────────────────────────────┘
            │
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃                          GROUND TRUTH DATA                                           ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

                         ┌──────────────────────────────┐
                         │ Ground Truth Target Text     │
                         │ (Human translation, Xhosa)   │
                         │ "Ndabona amagama amabini,    │
                         │  athunyelwe kum..."          │
                         └──────────────────────────────┘

                         ┌──────────────────────────────┐
                         │ Ground Truth Target Audio    │
                         │ (Native speaker, Xhosa)      │
                         │ (16kHz WAV)                  │
                         └──────────────────────────────┘

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃                          EVALUATION METRICS                                          ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

┌─────────────────────────────────────────────────────────────────────────────────────┐
│ METRIC 1: BLEU (Word-level n-gram precision)                                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│ INPUT:                                                                              │
│   ✓ Predicted Target Text ──────┐                                                  │
│   ✓ Ground Truth Target Text ───┤                                                  │
│                                  ▼                                                  │
│                          [ SacreBLEU ]                                              │
│                         Compare 1,2,3,4-grams                                       │
│                         Modified precision                                          │
│                         Brevity penalty                                             │
│                                  │                                                  │
│ OUTPUT: BLEU Score (0-100) ◄─────┘                                                  │
│         Higher = better word-level match                                            │
└─────────────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────────────┐
│ METRIC 2: chrF (Character-level n-gram F-score)                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│ INPUT:                                                                              │
│   ✓ Predicted Target Text ──────┐                                                  │
│   ✓ Ground Truth Target Text ───┤                                                  │
│                                  ▼                                                  │
│                          [ SacreBLEU chrF ]                                         │
│                         Extract char 1-6 grams                                      │
│                         F-score (precision + recall)                                │
│                                  │                                                  │
│ OUTPUT: chrF Score (0-100) ◄─────┘                                                  │
│         Higher = better character overlap                                           │
│         Better for morphologically rich languages                                   │
└─────────────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────────────┐
│ METRIC 3: COMET (Neural semantic quality)                                          │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│ INPUT (3-way comparison):                                                          │
│   ✓ Source Text ─────────────────┐                                                 │
│   ✓ Predicted Target Text ───────┤                                                 │
│   ✓ Ground Truth Target Text ────┤                                                 │
│                                   ▼                                                 │
│                    [ COMET Neural Model ]                                           │
│                    McGill-NLP/ssa-comet-qe                                          │
│                    Multilingual encoder                                             │
│                    Regression on quality                                            │
│                                   │                                                 │
│ OUTPUT: COMET Score (0-1) ◄───────┘                                                 │
│         Higher = better semantic preservation                                       │
│         Rewards valid paraphrases                                                   │
└─────────────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────────────┐
│ METRIC 4: MCD (Acoustic distance)                                                  │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│ INPUT (audio only):                                                                │
│   ✓ Predicted Target Audio ─────┐                                                  │
│   ✓ Ground Truth Target Audio ──┤                                                  │
│                                  ▼                                                  │
│                    [ MCD Computation ]                                              │
│                    1. Extract MFCCs (13 coefficients)                               │
│                    2. Dynamic Time Warping alignment                                │
│                    3. Euclidean distance per frame                                  │
│                    4. Average across frames                                         │
│                                  │                                                  │
│ OUTPUT: MCD (dB) ◄───────────────┘                                                  │
│         Lower = more acoustically similar                                           │
│         < 4.5 dB = Excellent                                                        │
│         Measures: voice quality, pronunciation, prosody                             │
│         Does NOT measure: semantic correctness                                      │
└─────────────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────────────┐
│ METRIC 5: BLASER 2.0 (Speech-to-speech translation quality)                        │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│ INPUT:                                                                              │
│   ✓ Source Audio (English) ─────┐                                                  │
│   ✓ Target Audio (Efik/etc) ────┤                                                  │
│                                  ▼                                                  │
│              ┌─────────────────────────────────┐                                    │
│              │   SONAR Speech Encoders         │                                    │
│              │   (Fine-tuned for each lang)    │                                    │
│              ├─────────────────────────────────┤                                    │
│              │ English Speech Encoder:         │                                    │
│              │   Source Audio → [src_emb]      │                                    │
│              │                                 │                                    │
│              │ Target Speech Encoder:          │                                    │
│              │   Target Audio → [mt_emb]       │                                    │
│              │   (Efik/Igbo/Swahili/Xhosa)     │                                    │
│              └────────────┬────────────────────┘                                    │
│                           │                                                         │
│                           ▼                                                         │
│              ┌─────────────────────────────────┐                                    │
│              │   BLASER 2.0 QE Model           │                                    │
│              │                                 │                                    │
│              │   score = f(src_emb, mt_emb)    │                                    │
│              └────────────┬────────────────────┘                                    │
│                           │                                                         │
│ OUTPUT: BLASER Score (0-5) ◄                                                        │
│         Higher = better speech translation                                          │
│                                                                                     │
│ MEASURES:                                                                           │
│   • Semantic preservation between source and target speech                         │
│   • Acoustic quality and naturalness                                               │
│   • Overall speech-to-speech translation quality                                   │
│                                                                                     │
│ HOW IT WORKS:                                                                       │
│   Speech encoders were fine-tuned to align speech with multilingual text           │
│   embeddings, creating a shared semantic space. BLASER compares speech              │
│   embeddings in this space to evaluate translation quality.                        │
└─────────────────────────────────────────────────────────────────────────────────────┘

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃                          FINAL OUTPUTS                                               ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

Per-Sample Results:                     Aggregate Results:
├─ UUID                                 ├─ Mean BLEU: 45.3
├─ Language pair                        ├─ Mean chrF: 62.7
├─ BLEU score                           ├─ Mean COMET: 0.68
├─ chrF score                           ├─ Mean MCD: 5.2 dB
├─ COMET score                          ├─ Mean BLASER: 3.4
├─ MCD score                            ├─ Score statistics (std, min, max)
└─ BLASER score                         └─ Sample counts

Saved to:
  • results/{execution_id}/{language}/detailed_results.csv
  • results/{execution_id}/{language}/summary.json
  • results/{execution_id}/overall_summary.json
```

---

## Metric Selection Guide: Which Metrics to Use?

```
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃            WHAT ARE YOU EVALUATING?                                       ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

START: What model/component do you want to evaluate?
  │
  ├─► [NLLB Translation Model] ──────────────────────────────────────────┐
  │   (Text → Text)                                                      │
  │                                                                      │
  │   Use These Metrics:                                                 │
  │   ✓ BLEU    - Word-level accuracy (exact matches)                   │
  │   ✓ chrF    - Character-level (better for agglutinative langs)      │
  │   ✓ COMET   - Semantic quality (meaning preservation)                │
  │                                                                      │
  │   Data Needed:                                                       │
  │   • Source Text (English)                                            │
  │   • Predicted Target Text (from NLLB)                                │
  │   • Ground Truth Target Text                                         │
  │                                                                      │
  │   Command:                                                           │
  │   python evaluation.py --mode predictions --metrics bleu chrf comet  │
  │                                                                      │
  ├─► [XTTS TTS Model] ───────────────────────────────────────────────────┤
  │   (Text → Audio)                                                     │
  │                                                                      │
  │   Use This Metric:                                                   │
  │   ✓ MCD     - Acoustic similarity to ground truth                    │
  │                                                                      │
  │   Data Needed:                                                       │
  │   • Predicted Target Audio (from XTTS)                               │
  │   • Ground Truth Target Audio                                        │
  │                                                                      │
  │   Command:                                                           │
  │   python evaluation.py --mode predictions --metrics mcd              │
  │                                                                      │
  ├─► [Full Pipeline: NLLB + XTTS] ───────────────────────────────────────┤
  │   (Text → Text → Audio)                                              │
  │                                                                      │
  │   Use All Text + Audio Metrics:                                      │
  │   ✓ BLEU    - Translation word accuracy                              │
  │   ✓ chrF    - Translation character accuracy                         │
  │   ✓ COMET   - Translation semantic quality                           │
  │   ✓ MCD     - TTS acoustic quality                                   │
  │                                                                      │
  │   Data Needed:                                                       │
  │   • Source Text                                                      │
  │   • Predicted Target Text (NLLB output)                              │
  │   • Predicted Target Audio (XTTS output)                             │
  │   • Ground Truth Target Text                                         │
  │   • Ground Truth Target Audio                                        │
  │                                                                      │
  │   Command:                                                           │
  │   python evaluation.py --mode predictions \                          │
  │     --metrics bleu chrf comet mcd                                    │
  │                                                                      │
  └─► [End-to-End Speech Translation] ────────────────────────────────────┤
      (English Audio → Xhosa Audio)                                      │
      Whisper → NLLB → XTTS                                              │
                                                                         │
      Use BLASER (Multimodal Metric):                                    │
      ✓ BLASER  - Complete speech-to-speech quality                      │
                                                                         │
      Optionally Also Use (for detailed analysis):                       │
      • BLEU, chrF, COMET (for intermediate text quality)                │
      • MCD (for audio quality component)                                │
                                                                         │
      Data Needed:                                                       │
      • Source Audio (English, from Whisper)                             │
      • Predicted Target Audio (Xhosa, from XTTS)                        │
      • Ground Truth Target Text (for semantic anchor)                   │
                                                                         │
      Command:                                                           │
      python evaluation.py --mode predictions --metrics blaser           │
                                                                         │
      OR for complete analysis:                                          │
      python evaluation.py --mode predictions \                          │
        --metrics bleu chrf comet mcd blaser                             │

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃            DIAGNOSTIC USE CASES                                           ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

Problem: "My BLEU scores are low but users say translations are good"
  │
  └─► Check COMET scores
      • High COMET = meaning is preserved, just different wording
      • BLEU penalizes valid paraphrases
      • chrF might also be higher (character-level similarity)

Problem: "Text metrics are high but audio sounds wrong"
  │
  └─► Check MCD and BLASER
      • High MCD = acoustic mismatch (voice quality, prosody)
      • Low BLASER = overall speech translation issues
      • Issue is in TTS model, not translation

Problem: "Which metric best represents overall quality?"
  │
  ├─► For TEXT translation: Use COMET (semantic quality)
  ├─► For AUDIO quality: Use MCD (acoustic similarity)
  └─► For SPEECH-TO-SPEECH: Use BLASER (end-to-end multimodal)

Problem: "Need to compare models"
  │
  └─► Use ALL metrics for comprehensive comparison:
      • BLEU - Standard benchmark (for papers)
      • chrF - Better for morphologically rich languages
      • COMET - Best semantic quality measure
      • MCD - Audio fidelity
      • BLASER - Overall speech translation

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃            METRIC COMPATIBILITY MATRIX                                    ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

Model Output Type │ BLEU │ chrF │ COMET │ MCD │ BLASER │
──────────────────┼──────┼──────┼───────┼─────┼────────┤
Text Only         │  ✓   │  ✓   │   ✓   │  ✗  │   ✗    │
Audio Only        │  ✗   │  ✗   │   ✗   │  ✓  │   ✗    │
Text + Audio      │  ✓   │  ✓   │   ✓   │  ✓  │   ✗    │
Speech-to-Speech  │  ✓*  │  ✓*  │   ✓*  │  ✓  │   ✓    │

* Requires intermediate text from NLLB model
```

---

## Metric Data Flow Diagrams

### 1. BLEU (Bilingual Evaluation Understudy)

**What it compares**: Word-level n-gram overlap

```
┌─────────────────────────────────────────────────────────────┐
│  BLEU Inputs                                                │
└─────────────────────────────────────────────────────────────┘

Predicted Target Text (from NLLB):
"Ndabona amagama amabini, athunyelwa kum yimimangaliso..."
                    │
                    │ Compare 1,2,3,4-word sequences
                    ▼
Ground Truth Target Text (human translation):
"Ndabona amagama amabini, athunyelwe kum ngemimangaliso..."

┌─────────────────────────────────────────────────────────────┐
│  How BLEU Works                                             │
└─────────────────────────────────────────────────────────────┘

1. Extract n-grams (1-word, 2-word, 3-word, 4-word sequences)
2. Count how many predicted n-grams appear in reference
3. Calculate precision for each n-gram size
4. Apply brevity penalty if prediction is shorter
5. Geometric mean of precisions → BLEU score

Score: 0-100 (higher = more exact word matches)
```

**Not used**: Source text, any audio
**Limitation**: Penalizes valid paraphrases

---

### 2. chrF (Character n-gram F-score)

**What it compares**: Character-level sequences

```
┌─────────────────────────────────────────────────────────────┐
│  chrF Inputs                                                │
└─────────────────────────────────────────────────────────────┘

Predicted Target Text (from NLLB):
"Ndabona amagama amabini, athunyelwa..."
                    │
                    │ Compare character sequences (1-6 chars)
                    ▼
Ground Truth Target Text:
"Ndabona amagama amabini, athunyelwe..."

┌─────────────────────────────────────────────────────────────┐
│  How chrF Works                                             │
└─────────────────────────────────────────────────────────────┘

1. Extract character n-grams: "Nd", "da", "ab", "bo", "on"...
2. Calculate precision: chars in prediction that match reference
3. Calculate recall: chars in reference that appear in prediction
4. F-score: harmonic mean of precision and recall

Score: 0-100 (higher = better character overlap)
```

**Not used**: Source text, any audio
**Advantage**: Better for morphologically rich/agglutinative languages (like Xhosa, Igbo)

---

### 3. COMET (Crosslingual Optimized Metric for Evaluation of Translation)

**What it compares**: Semantic quality using neural embeddings (3-way comparison)

```
┌─────────────────────────────────────────────────────────────┐
│  COMET Inputs (ALL THREE REQUIRED)                         │
└─────────────────────────────────────────────────────────────┘

Source Text (English):
"I saw two names, sent to me by the wonders of technology..."
                    │
                    ├──────────────────┐
                    │                  │
                    ▼                  ▼
Predicted Target Text:          Ground Truth Target Text:
"Ndabona amagama amabini,       "Ndabona amagama amabini,
athunyelwa kum yimimangaliso"   athunyelwe kum ngemimangaliso"

┌─────────────────────────────────────────────────────────────┐
│  How COMET Works                                            │
└─────────────────────────────────────────────────────────────┘

1. Encode source → multilingual embedding
2. Encode prediction → multilingual embedding
3. Encode reference → multilingual embedding
4. Neural regression model (McGill-NLP/ssa-comet-qe) predicts:
   - Is meaning preserved from source?
   - How fluent is the prediction?
   - How similar to reference?

Score: ~0-1 (higher = better semantic translation quality)
```

**Not used**: Any audio
**Advantage**: Captures meaning beyond surface form, rewards valid paraphrases
**Key insight**: Understands that "yimimangaliso" and "ngemimangaliso" both mean "wonders"

---

### 4. MCD (Mel-Cepstral Distance)

**What it compares**: Acoustic similarity of speech signals

```
┌─────────────────────────────────────────────────────────────┐
│  MCD Inputs (AUDIO ONLY)                                    │
└─────────────────────────────────────────────────────────────┘

Predicted Target Audio (from XTTS):
Segment=152899_User=11_Language=xho_pred.wav (16kHz)
                    │
                    │ Extract acoustic features
                    │ Compare frame-by-frame
                    ▼
Ground Truth Target Audio (native speaker):
Segment=152899_User=11_Language=xho.wav (16kHz)

┌─────────────────────────────────────────────────────────────┐
│  How MCD Works                                              │
└─────────────────────────────────────────────────────────────┘

1. Convert audio to spectrograms (frequency representation)
2. Extract MFCCs (Mel-Frequency Cepstral Coefficients):
   - 13 coefficients per frame (typically 25ms frames)
   - Captures voice timbre, pronunciation, prosody
3. Align frames using Dynamic Time Warping (handles speed differences)
4. Compute Euclidean distance between aligned MFCC vectors
5. Average distance across all frames

Score: Distance in dB (LOWER is better)
  • < 4.5 dB = Excellent (very similar)
  • 4.5-6.0 dB = Good
  • 6.0-8.0 dB = Fair
  • > 8.0 dB = Poor (very different)
```

**Not used**: Any text
**What it measures**: Voice quality, pronunciation accuracy, prosody
**What it does NOT measure**: Semantic correctness (audio could sound perfect but say wrong words!)

---

### 5. BLASER 2.0 (Bilingual Speech Translation Evaluation)

**What it compares**: Speech-to-speech translation quality (audio→audio with text anchor)

```
┌─────────────────────────────────────────────────────────────┐
│  BLASER Inputs: Source Audio + Target Audio + Reference Text│
└─────────────────────────────────────────────────────────────┘

┌────────────────────────┐          ┌────────────────────────┐
│ Source Audio (English) │          │ Source Text (English)  │
│ Segment=152899_User=11 │          │ "I saw two names..."   │
│ _Language=en_src.wav   │          │ (used for logging only)│
└────────────────────────┘          └────────────────────────┘
         │                                      │
         │ SONAR Speech Encoder                 │
         │ (Fine-tuned for English)             │
         ▼                                      │
    [src_emb]                                   │
    (1024-dim                                   │
     vector)                                    │
         │                                      │
         │                                      │
         │         BLASER Model                 │
         │      (Quality Estimator)             │
         │              │                       │
         └──────────────┼───────────────────────┘
                        │
         ┌──────────────┴──────────────────┐
         │                                 │
         ▼                                 ▼
    [mt_emb]                          [ref_emb]
    (1024-dim                         (1024-dim
     vector)                           vector)
         │                                 │
         │ SONAR Speech Encoder            │ SONAR Text Encoder
         │ (Fine-tuned for Xhosa)          │ (xho_Latn)
         │                                 │
         ▼                                 ▼
┌────────────────────────┐          ┌────────────────────────┐
│ Target Audio (Xhosa)   │          │ Reference Text (Xhosa) │
│ PREDICTED or GROUND    │          │ "Ndabona amagama..."   │
│ TRUTH .wav (16kHz)     │          │ (ground truth)         │
└────────────────────────┘          └────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│  How BLASER Works                                           │
└─────────────────────────────────────────────────────────────┘

1. Encode source audio → speech embedding (what was said)
2. Encode target audio → speech embedding (what translation sounds like)
3. Encode reference text → text embedding (what translation SHOULD mean)

4. BLASER neural model computes:
   score = f(src_emb, mt_emb, ref_emb)

   Measures:
   ✓ Does target audio semantically match source audio?
   ✓ Is target audio acoustically good quality?
   ✓ Does target audio convey the meaning in reference text?

Score: 0-5 (higher = better overall speech translation)
```

**Two BLASER Modes**:

| Mode | Inputs | Use Case |
|------|--------|----------|
| **QE** (Quality Estimation) | src_audio + target_audio | No reference needed, faster |
| **Ref** (Reference-based) | src_audio + target_audio + ref_text | More accurate, uses ground truth |

**Not used**: Predicted target text (BLASER only uses the AUDIO of the translation)

**Key Insight**: BLASER is the only metric that evaluates speech-to-speech translation end-to-end. It understands:
- Acoustic quality (like MCD)
- Semantic correctness (like COMET)
- Speech-specific issues (prosody, speaker characteristics)

---

## Data Structure

### Input Data Organization

```
services/data/languages/{language}/
├── nmt_predictions_{model_name}.csv          # NLLB outputs
│   Columns: segment_id | user_id | src_text | predicted_tgt_text | ground_truth_tgt_text | iso_code
│
├── predicted_tgt_audio_{tts_model}/          # XTTS outputs
│   └── Segment={id}_User={id}_Language={iso}_pred.wav
│
├── src_audio/                                # Whisper-generated English audio
│   └── Segment={id}_User={id}_Language=en_src.wav
│
├── processed_audio_normalized/               # Ground truth target audio
│   └── Segment={id}_User={id}_Language={iso}.wav
│
└── mapped_metadata_test.csv                  # Ground truth mappings
    Columns: segment_id | user_id | src_text | tgt_text | tgt_audio | iso_code
```

### What Each Metric Needs (Data Requirements)

| Metric | Source Text | Pred Text | GT Text | Source Audio | Pred Audio | GT Audio |
|--------|-------------|-----------|---------|--------------|------------|----------|
| BLEU   | ✗           | ✓         | ✓       | ✗            | ✗          | ✗        |
| chrF   | ✗           | ✓         | ✓       | ✗            | ✗          | ✗        |
| COMET  | ✓           | ✓         | ✓       | ✗            | ✗          | ✗        |
| MCD    | ✗           | ✗         | ✗       | ✗            | ✓          | ✓        |
| BLASER | ✗*          | ✗         | ✓       | ✓            | ✓          | ✗        |

*BLASER uses source text for logging only, not in computation

---

## Evaluation Workflow

### Step 1: Data Loading

For each language (efik, igbo, swahili, xhosa):

1. Load NMT predictions CSV: `nmt_predictions_{nmt_model}.csv`
2. Load ground truth CSV: `mapped_metadata_test.csv`
3. Match rows by `segment_id + user_id`
4. Locate audio files:
   - Source audio: `src_audio/Segment={id}_User={id}_Language=en_src.wav`
   - Predicted audio: `predicted_tgt_audio_{tts_model}/Segment={id}_User={id}_Language={iso}_pred.wav`
   - Ground truth audio: `processed_audio_normalized/Segment={id}_User={id}_Language={iso}.wav`
5. Create `TranslationSample` objects with all data

### Step 2: Sample Validation

Each sample is validated for required data based on requested metrics:
- Skip samples missing required files
- Log warnings for invalid samples

### Step 3: Metric Computation

```
For each language:
┌────────────────────────────────────────────┐
│ 1. Batch Text Metrics                     │
│    BLEU(pred_text, gt_text)               │
│    chrF(pred_text, gt_text)               │
│    COMET(src_text, pred_text, gt_text)    │
├────────────────────────────────────────────┤
│ 2. Batch Audio Metrics                    │
│    MCD(pred_audio, gt_audio)              │
│    → Pairwise comparison for each sample  │
├────────────────────────────────────────────┤
│ 3. Batch Multimodal Metrics               │
│    BLASER(src_audio, pred_audio, gt_text) │
│    → Uses SONAR embeddings                │
└────────────────────────────────────────────┘
```

### Step 4: Results Aggregation

Per-sample results:
- UUID, language pair
- Individual scores for each metric

Aggregate results:
- Corpus-level scores (mean across all samples)
- Score statistics (std, min, max, median)
- Sample counts (total, valid, skipped)

---

## Running Evaluations

### Predictions Mode

Evaluates NLLB + XTTS outputs against ground truth:

```bash
python evaluation.py \
  --mode predictions \
  --language xhosa \
  --data-dir /path/to/data/languages \
  --nmt-model multilang_finetuned_final \
  --tts-model MULTILINGUAL_TRAINING_11_5-November-05-2025_10+57AM-cc09632 \
  --metrics bleu chrf comet mcd blaser \
  --output-dir ./results \
  --execution-id eval_20251201
```

### Ground Truth Mode

Measures reference data quality (sanity check):

```bash
python evaluation.py \
  --mode ground_truth \
  --type audio_to_audio \
  --data-dir /path/to/app_evaluation/audio_to_audio \
  --metrics bleu chrf comet blaser
```

**Note**: MCD is skipped in ground truth mode (would compare audio to itself)

---

## Output Structure

```
results/{execution_id}/
├── manifest.json                    # Execution metadata
├── overall_summary.json             # Cross-language statistics
├── visualizations/
│   └── language_metric_heatmap.png
│
├── {language}/                      # Per-language results
│   ├── summary.json                 # Aggregate scores only
│   ├── detailed_results.csv         # Per-sample scores (tabular)
│   ├── per_sample_results.json      # Full sample details
│   ├── logs/
│   │   └── evaluation.log
│   └── visualizations/
│       ├── metric_distributions.png
│       ├── score_correlations.png
│       └── quality_breakdown.png
│
└── summary_report.html              # Interactive HTML report
```

---

## Metric Interpretation Guide

### Quality Thresholds

| Metric | Excellent | Good | Fair | Poor |
|--------|-----------|------|------|------|
| BLEU | > 50 | 30-50 | 15-30 | < 15 |
| chrF | > 70 | 50-70 | 30-50 | < 30 |
| COMET | > 0.7 | 0.5-0.7 | 0.3-0.5 | < 0.3 |
| MCD | < 4.5 dB | 4.5-6.0 dB | 6.0-8.0 dB | > 8.0 dB |
| BLASER | > 4.0 | 3.0-4.0 | 2.0-3.0 | < 2.0 |

### Diagnostic Patterns

| Pattern | Diagnosis | Likely Issue |
|---------|-----------|--------------|
| Low BLEU, High chrF | Captures char patterns but not exact words | Word choice errors, valid paraphrases |
| High BLEU, Low COMET | Exact words but wrong meaning | Literal translation, idiom errors |
| High text metrics, Low MCD | Good translation but poor TTS | TTS quality issues (prosody, voice) |
| High MCD, High BLASER | Bad TTS but meaning preserved | Audio quality vs semantic trade-off |
| High text metrics, Low BLASER | Good text but speech issues | Pronunciation, speaking rate problems |

---

## Important Implementation Details

### Audio Requirements
- **Sample rate**: 16kHz (SONAR requirement)
- **Format**: Mono WAV
- **Preprocessing**: Use `scripts/resample_audio_samples.py`

### Language Support
- **Target languages**: Efik (efi), Igbo (ibo), Swahili (swh), Xhosa (xho)
- **BLASER encoders**: Fine-tuned SONAR speech encoders per language
- **Text encoding**: Efik uses Igbo proxy (SONAR text encoder doesn't support Efik natively)

### BLASER Fine-tuned Encoders
Located in `blaser/checkpoints/`:
- `finetuned_efi/` - Efik (base: English encoder + fine-tuning)
- `finetuned_ibo/` - Igbo (base: English encoder + fine-tuning)
- `finetuned_swh/` - Swahili (base: Swahili encoder + fine-tuning)
- `finetuned_xho/` - Xhosa (base: Swahili encoder + fine-tuning)

---

## Architecture Components

| File | Purpose |
|------|---------|
| [evaluation.py](evaluation.py) | CLI entry point, orchestration |
| [pipeline.py](pipeline.py) | EvaluationPipeline class, metric computation |
| [scripts/data_loader.py](scripts/data_loader.py) | CSV loading, sample validation |
| [scripts/text_metrics.py](scripts/text_metrics.py) | BLEU, chrF (SacreBLEU) |
| [scripts/audio_metrics.py](scripts/audio_metrics.py) | MCD computation |
| [scripts/comet_evaluator.py](scripts/comet_evaluator.py) | COMET wrapper |
| [scripts/blaser_evaluator.py](scripts/blaser_evaluator.py) | BLASER subprocess wrapper |
| [blaser/evaluate.py](blaser/evaluate.py) | BLASER 2.0 core implementation |
| [scripts/visualizations.py](scripts/visualizations.py) | Result visualization |

### Key Dependencies
- **SacreBLEU**: Text metrics (BLEU, chrF)
- **COMET**: Neural translation metric
- **mel-cepstral-distance**: MCD computation
- **SONAR**: Multimodal embeddings for BLASER
- **PyTorch**: Deep learning backend
