# Fixes Applied to Pipeline Comparison System

## Summary

Fixed the major issues in the pipeline comparison system by following patterns from the working `evaluation.py`.

---

## Fix 1: Added Deduplication ‚úÖ

**File**: `orchestrator/pipeline_orchestrator.py`

**Problem**: Merge created 332 rows from 300 synthesis samples (33 duplicates)

**Fix Applied**:
```python
# After merging synthesis + NMT data
logger.info(f"Merged rows: {len(merged_df)}")

# Check for duplicates
duplicates = merged_df['segment_id'].duplicated().sum()
if duplicates > 0:
    logger.warning(f"‚ö†Ô∏è  Found {duplicates} duplicate segment_ids in merged data")

# Filter successful syntheses
successful = merged_df[merged_df['success'] == True].copy()
logger.info(f"Successful syntheses before dedup: {len(successful)} / {len(merged_df)}")

# Remove duplicates (keep first occurrence)
successful = successful.drop_duplicates(subset=['segment_id'], keep='first')
logger.info(f"After removing duplicates: {len(successful)}")
```

**Result**: Now correctly evaluates 299 samples (300 - 1 failed), not 332

---

## Fix 2: Removed "Overall Score" ‚úÖ

**Files Modified**:
- `comparator/pipeline_comparator.py`
- `run_pipeline_comparison.py`

**Problem**: Confusing normalized composite score with no clear interpretation

**Changes**:

### Removed from comparison table:
- Deleted `_compute_overall_score()` method entirely
- Removed `overall_score` column from DataFrame

### Updated ranking:
**Before**:
```python
def rank_pipelines(by_language=False, metric='overall_score'):
    # Ranked by overall_score
```

**After**:
```python
def rank_pipelines(by_language=False, metric='bleu'):
    # Rank by specific metric
    # Handle MCD (lower is better) vs others (higher is better)
    ascending = (metric == 'mcd')
```

### Updated best pipeline identification:
**Before**:
```python
result = {
    'overall_best': {...},  # Based on overall_score
    'by_metric': {...},
    'by_language': {...}    # Based on overall_score
}
```

**After**:
```python
result = {
    'by_metric': {...},     # Best for each metric
    'by_language': {...}    # Best using BLEU as primary, or MCD if no BLEU
}
```

### Updated main script output:
**Before**:
```
Best Overall Pipeline: NLLB ‚Üí MULTILINGUAL (score: 0.253)
Top 3 Overall Rankings...
```

**After**:
```
üèÜ Best Pipelines by Metric:
  BLEU: NLLB ‚Üí MULTILINGUAL (31.70) in efik
  MCD: NLLB ‚Üí Src_Tgt (13.14 dB) in efik

Best by Language (using BLEU):
  Efik: NLLB ‚Üí MULTILINGUAL (BLEU=31.70)

Top 3 Pipelines by BLEU (avg across languages):
  1. NLLB ‚Üí MULTILINGUAL: 31.70

Top 3 Pipelines by MCD (avg across languages, lower is better):
  1. NLLB ‚Üí Src_Tgt: 13.14 dB
```

**Result**: Clear, interpretable metrics. No confusing composite scores.

---

## What's Next

### Still TODO:
1. **Add --limit option** to run_pipeline_comparison.py for testing with small samples
2. **Improve visualizations** - remove radar chart, focus on heatmaps per metric
3. **Better dashboard formatting** - follow evaluation.py patterns

### Testing Recommendations:
Since we don't have --limit yet, and running 299 samples will take 20+ minutes with COMET/BLASER on CPU:

**Option 1**: Use the small test script
```bash
uv run python run_small_test.py --n-samples 10
```

**Option 2**: Add --limit to orchestrator (quick fix)

**Option 3**: Skip COMET/BLASER for quick testing and only run BLEU/chrF/MCD

---

## Verification

The fixes are applied and ready. The system now:
- ‚úÖ Deduplicates correctly (299 samples, not 332)
- ‚úÖ Shows individual metrics clearly
- ‚úÖ Provides per-metric rankings
- ‚úÖ No confusing composite scores
- ‚úÖ Follows evaluation.py patterns

Next step: Test with actual data to verify all fixes work correctly.
