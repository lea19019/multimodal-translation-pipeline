=========================================================
SLURM JOB ID: 8326409
JOB NAME: train_efik
Running on nodes: cs-1-1
Number of GPUs allocated: 1
=========================================================
Activating environment...
--- Python and Torch Diagnostics ---
/usr/bin/python
PyTorch version: 2.8.0+cu128
CUDA available: True
Number of GPUs: 1
------------------------------------
--- GPU Diagnostics ---
Tue Nov  4 21:00:14 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:8A:00.0 Off |                    0 |
| N/A   28C    P0             56W /  400W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
-----------------------
Setting up NLLB model...
PyTorch version: 2.8.0+cu128
CUDA available: True
Loading model from local path: ./checkpoints/base
Map:   0%|          | 0/34895 [00:00<?, ? examples/s]/home/vacl2/multimodal_translation/services/nmt/.venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map:   3%|▎         | 1000/34895 [00:00<00:04, 7685.18 examples/s]Map:   6%|▌         | 2000/34895 [00:00<00:04, 8019.33 examples/s]Map:   9%|▊         | 3000/34895 [00:00<00:03, 8394.36 examples/s]Map:  11%|█▏        | 4000/34895 [00:00<00:03, 8958.32 examples/s]Map:  17%|█▋        | 6000/34895 [00:00<00:02, 9879.70 examples/s]Map:  20%|██        | 7000/34895 [00:00<00:02, 9797.86 examples/s]Map:  23%|██▎       | 8000/34895 [00:00<00:02, 9352.87 examples/s]Map:  26%|██▌       | 9000/34895 [00:01<00:02, 8752.11 examples/s]Map:  29%|██▊       | 10000/34895 [00:01<00:03, 6255.80 examples/s]Map:  32%|███▏      | 11000/34895 [00:01<00:03, 6730.14 examples/s]Map:  34%|███▍      | 12000/34895 [00:01<00:03, 7174.21 examples/s]Map:  37%|███▋      | 13000/34895 [00:01<00:02, 7340.93 examples/s]Map:  40%|████      | 14000/34895 [00:01<00:02, 7690.38 examples/s]Map:  43%|████▎     | 15000/34895 [00:01<00:02, 7741.85 examples/s]Map:  46%|████▌     | 16000/34895 [00:02<00:02, 7674.58 examples/s]Map:  49%|████▊     | 17000/34895 [00:02<00:02, 7648.60 examples/s]Map:  52%|█████▏    | 18000/34895 [00:02<00:02, 7839.84 examples/s]Map:  54%|█████▍    | 19000/34895 [00:02<00:02, 7846.04 examples/s]Map:  57%|█████▋    | 20000/34895 [00:02<00:01, 7779.83 examples/s]Map:  60%|██████    | 21000/34895 [00:02<00:01, 7777.69 examples/s]Map:  63%|██████▎   | 22000/34895 [00:02<00:01, 7885.65 examples/s]Map:  66%|██████▌   | 23000/34895 [00:02<00:01, 8000.48 examples/s]Map:  69%|██████▉   | 24000/34895 [00:03<00:01, 8267.36 examples/s]Map:  72%|███████▏  | 25000/34895 [00:03<00:01, 6194.06 examples/s]Map:  75%|███████▍  | 26000/34895 [00:03<00:01, 6806.66 examples/s]Map:  77%|███████▋  | 27000/34895 [00:03<00:01, 7270.51 examples/s]Map:  80%|████████  | 28000/34895 [00:03<00:00, 7654.84 examples/s]Map:  83%|████████▎ | 29000/34895 [00:03<00:00, 7928.96 examples/s]Map:  86%|████████▌ | 30000/34895 [00:03<00:00, 8092.06 examples/s]Map:  89%|████████▉ | 31000/34895 [00:03<00:00, 8424.46 examples/s]Map:  95%|█████████▍| 33000/34895 [00:04<00:00, 8753.13 examples/s]Map:  97%|█████████▋| 34000/34895 [00:04<00:00, 8515.11 examples/s]Map: 100%|██████████| 34895/34895 [00:04<00:00, 8413.85 examples/s]Map: 100%|██████████| 34895/34895 [00:04<00:00, 7913.12 examples/s]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Traceback (most recent call last):
  File "/home/vacl2/multimodal_translation/services/nmt/nllb_nmt.py", line 75, in <module>
    trainer = Seq2SeqTrainer(
  File "/home/vacl2/multimodal_translation/services/nmt/.venv/lib64/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/vacl2/multimodal_translation/services/nmt/.venv/lib64/python3.9/site-packages/transformers/trainer_seq2seq.py", line 72, in __init__
    super().__init__(
  File "/home/vacl2/multimodal_translation/services/nmt/.venv/lib64/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/vacl2/multimodal_translation/services/nmt/.venv/lib64/python3.9/site-packages/transformers/trainer.py", line 688, in __init__
    self.callback_handler = CallbackHandler(
  File "/home/vacl2/multimodal_translation/services/nmt/.venv/lib64/python3.9/site-packages/transformers/trainer_callback.py", line 449, in __init__
    self.add_callback(cb)
  File "/home/vacl2/multimodal_translation/services/nmt/.venv/lib64/python3.9/site-packages/transformers/trainer_callback.py", line 466, in add_callback
    cb = callback() if isinstance(callback, type) else callback
  File "/home/vacl2/multimodal_translation/services/nmt/.venv/lib64/python3.9/site-packages/transformers/integrations/integration_utils.py", line 683, in __init__
    raise RuntimeError(
RuntimeError: TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX.
=========================================================
Training script finished.
=========================================================
