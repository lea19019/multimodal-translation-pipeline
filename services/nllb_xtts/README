# End-to-End Differentiable Translation-to-Speech Pipeline

## Project Overview

**Goal:** Train NLLB (translation) and XTTS (text-to-speech) jointly so translations are optimized for speech synthesis quality.

**Key Innovation:** Use Gumbel-Softmax reparameterization to make the discrete token sampling differentiable, enabling gradients to flow from audio quality back to translation decisions.

---

## The Core Problem & Solution

### Problem: Discrete Bottleneck
```
NLLB → [DISCRETE TOKENS] → XTTS → audio
         ↑ Can't backprop here!
```

### Solution: Gumbel-Softmax (VAE-style reparameterization)
```
NLLB → [SOFT TOKEN DISTRIBUTIONS] → XTTS → audio
         ↑ Gradients flow through!
```

---

## Key Concepts You'll Need

### 1. Gumbel-Softmax Reparameterization
**What it does:** Makes discrete sampling differentiable

**Normal sampling (not differentiable):**
```python
# Pick one token (discrete choice)
token_id = torch.argmax(logits)  # or sample from categorical
```

**Gumbel-Softmax (differentiable):**
```python
# Get soft distribution over all tokens
def gumbel_softmax(logits, temperature):
    gumbels = -torch.log(-torch.log(torch.rand_like(logits)))
    y = logits + gumbels
    return F.softmax(y / temperature, dim=-1)

# Result: [0.05, 0.02, 0.87, 0.01, ...] instead of [0, 0, 1, 0, ...]
```

**Key parameter - Temperature (τ):**
- High τ (e.g., 5.0): Very soft, almost uniform → gradients flow easily
- Low τ (e.g., 0.1): Almost discrete, sharp → closer to real inference
- **Strategy:** Start high, anneal down during training

**Resources:**
- Original paper: "Categorical Reparameterization with Gumbel-Softmax" (Jang et al., 2016)
- Similar to: VAE reparameterization trick, straight-through estimators

---

### 2. Soft Token Embeddings
**Problem:** XTTS expects discrete token IDs, we have soft distributions

**Solution:** Use weighted average of embeddings
```python
# Normal (discrete):
token_id = 5
embedding = embedding_table[token_id]  # Shape: [hidden_dim]

# Soft (differentiable):
soft_dist = [0.05, 0.02, 0.87, 0.01, ...]  # Shape: [vocab_size]
soft_embedding = soft_dist @ embedding_table  # Shape: [hidden_dim]
# This is: 0.05 * emb[0] + 0.02 * emb[1] + 0.87 * emb[2] + ...
```

---

### 3. Joint Loss Function
**We optimize for both translation quality AND audio quality**
```python
total_loss = α * translation_loss + β * audio_loss

# Translation loss: Standard cross-entropy
# Compare NLLB output to reference translation

# Audio loss: Could be:
# - MSE between generated and reference audio
# - Mel-Cepstral Distortion (MCD)
# - Perceptual loss
# - Or combination
```

**Loss weighting strategies:**
- Fixed: α=0.5, β=0.5
- Dynamic: Start α=0.9, β=0.1 → gradually shift to α=0.5, β=0.5
- Uncertainty weighting: Learn α, β as trainable parameters

---

### 4. Temperature Annealing Schedule
**Why:** Start soft for learning, end sharp for quality
```python
# Linear annealing
temperature = max_temp - (max_temp - min_temp) * (step / total_steps)

# Exponential annealing
temperature = min_temp + (max_temp - min_temp) * exp(-step / decay_rate)

# Typical values:
# max_temp = 5.0  (very soft)
# min_temp = 0.5  (almost discrete)
```

---

## Architecture Overview

### Standard Pipeline (Independent Training)
```
Training NLLB:
Source text → NLLB → Target text
                      ↓
                   Compare to reference → Loss → Update NLLB

Training XTTS:
Target text → XTTS → Audio
                      ↓
                   Compare to reference → Loss → Update XTTS
```

### Our Joint Pipeline
```
Source text 
    ↓
NLLB Encoder → hidden states
    ↓
NLLB Decoder → logits for each position
    ↓
Gumbel-Softmax (τ) → soft token distributions
    ↓
Soft Embedding Lookup → continuous embeddings
    ↓
XTTS Model → synthesized audio
    ↓
Loss Computation:
  - Translation loss (compare soft tokens to reference)
  - Audio loss (compare audio to reference)
    ↓
Backprop through BOTH models
    ↓
Update NLLB + XTTS weights
```

---

## Implementation Roadmap

### Phase 1: Setup & Modification (6-8 hours)

#### Step 1.1: Modify NLLB Decoder
**File to modify:** `models/nllb_wrapper.py` (or wherever your NLLB lives)

**What to add:**
```python
class GumbelNLLBDecoder(nn.Module):
    def __init__(self, base_decoder, vocab_size):
        self.decoder = base_decoder
        self.vocab_size = vocab_size
    
    def forward(self, encoder_hidden, temperature=1.0, use_gumbel=True):
        # Get logits from decoder
        logits = self.decoder(encoder_hidden)  # Shape: [batch, seq_len, vocab_size]
        
        if use_gumbel and self.training:
            # Apply Gumbel-Softmax
            soft_tokens = gumbel_softmax(logits, temperature)
            return soft_tokens  # Shape: [batch, seq_len, vocab_size]
        else:
            # Standard inference (discrete)
            token_ids = torch.argmax(logits, dim=-1)
            return token_ids
```

**Key things to track:**
- Shape changes: `[batch, seq_len]` → `[batch, seq_len, vocab_size]`
- When to use soft vs hard (training vs inference)
- Temperature as a parameter

---

#### Step 1.2: Modify XTTS Input Layer
**File to modify:** `models/xtts_wrapper.py`

**What to add:**
```python
class SoftTokenXTTS(nn.Module):
    def __init__(self, base_xtts):
        self.xtts = base_xtts
        self.embedding_table = self.xtts.get_embedding_layer()
    
    def forward(self, input_tokens, speaker_wav):
        if input_tokens.dim() == 3:  # Soft tokens [batch, seq_len, vocab_size]
            # Convert soft distribution to soft embedding
            soft_embeddings = torch.matmul(
                input_tokens,  # [batch, seq_len, vocab_size]
                self.embedding_table.weight  # [vocab_size, hidden_dim]
            )  # Result: [batch, seq_len, hidden_dim]
            
            # Feed soft embeddings to XTTS
            audio = self.xtts.forward_with_embeddings(soft_embeddings, speaker_wav)
        else:  # Discrete tokens [batch, seq_len]
            # Standard path
            audio = self.xtts(input_tokens, speaker_wav)
        
        return audio
```

**Challenges to expect:**
- XTTS might not have `forward_with_embeddings` - you may need to hack the forward pass
- Look for where XTTS does `self.embedding(input_ids)` and replace with your soft embeddings
- Keep track of shapes carefully

---

#### Step 1.3: Create Joint Model Wrapper
**File to create:** `models/joint_pipeline.py`
```python
class JointTranslationTTS(nn.Module):
    def __init__(self, nllb_model, xtts_model):
        super().__init__()
        self.nllb = GumbelNLLBDecoder(nllb_model)
        self.xtts = SoftTokenXTTS(xtts_model)
        self.temperature = 5.0  # Will be updated during training
    
    def forward(self, source_ids, target_ids, target_audio, speaker_wav):
        # Encode source
        encoder_output = self.nllb.encode(source_ids)
        
        # Decode with Gumbel-Softmax
        soft_tokens = self.nllb.decode_soft(
            encoder_output, 
            temperature=self.temperature
        )  # [batch, seq_len, vocab_size]
        
        # Synthesize audio
        generated_audio = self.xtts(soft_tokens, speaker_wav)
        
        # Compute losses
        translation_loss = self.compute_translation_loss(soft_tokens, target_ids)
        audio_loss = self.compute_audio_loss(generated_audio, target_audio)
        
        return {
            'translation_loss': translation_loss,
            'audio_loss': audio_loss,
            'soft_tokens': soft_tokens,
            'generated_audio': generated_audio
        }
    
    def compute_translation_loss(self, soft_tokens, target_ids):
        # Cross-entropy between soft distribution and hard target
        # soft_tokens: [batch, seq_len, vocab_size]
        # target_ids: [batch, seq_len]
        log_probs = torch.log(soft_tokens + 1e-10)
        loss = F.nll_loss(
            log_probs.view(-1, log_probs.size(-1)),
            target_ids.view(-1),
            ignore_index=self.tokenizer.pad_token_id
        )
        return loss
    
    def compute_audio_loss(self, generated_audio, target_audio):
        # Simple MSE (you can use MCD or other metrics)
        # Align lengths first
        min_len = min(len(generated_audio), len(target_audio))
        gen = generated_audio[:min_len]
        tgt = target_audio[:min_len]
        
        return F.mse_loss(gen, tgt)
```

---

### Phase 2: Training Loop (4-6 hours)

#### Step 2.1: Temperature Scheduler
**File to create:** `training/schedulers.py`
```python
class TemperatureScheduler:
    def __init__(self, max_temp=5.0, min_temp=0.5, total_steps=10000):
        self.max_temp = max_temp
        self.min_temp = min_temp
        self.total_steps = total_steps
        self.current_step = 0
    
    def step(self):
        self.current_step += 1
        # Linear annealing
        progress = min(1.0, self.current_step / self.total_steps)
        temp = self.max_temp - (self.max_temp - self.min_temp) * progress
        return temp
    
    def get_temperature(self):
        return self.max_temp - (self.max_temp - self.min_temp) * min(1.0, self.current_step / self.total_steps)
```

---

#### Step 2.2: Main Training Loop
**File to create:** `train_joint.py`
```python
def train_joint_model(model, train_loader, val_loader, config):
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)
    temp_scheduler = TemperatureScheduler(
        max_temp=config.max_temp,
        min_temp=config.min_temp,
        total_steps=config.total_steps
    )
    
    # Loss weights (can be learned or fixed)
    alpha = config.translation_weight  # e.g., 0.6
    beta = config.audio_weight         # e.g., 0.4
    
    for epoch in range(config.num_epochs):
        model.train()
        
        for batch_idx, batch in enumerate(train_loader):
            # Update temperature
            current_temp = temp_scheduler.step()
            model.temperature = current_temp
            
            # Forward pass
            outputs = model(
                source_ids=batch['source_ids'],
                target_ids=batch['target_ids'],
                target_audio=batch['target_audio'],
                speaker_wav=batch['speaker_wav']
            )
            
            # Combined loss
            total_loss = (alpha * outputs['translation_loss'] + 
                         beta * outputs['audio_loss'])
            
            # Backward
            optimizer.zero_grad()
            total_loss.backward()
            
            # Gradient clipping (important!)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # Logging
            if batch_idx % 100 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}")
                print(f"  Temperature: {current_temp:.3f}")
                print(f"  Translation Loss: {outputs['translation_loss'].item():.4f}")
                print(f"  Audio Loss: {outputs['audio_loss'].item():.4f}")
                print(f"  Total Loss: {total_loss.item():.4f}")
        
        # Validation
        if epoch % config.val_every == 0:
            val_metrics = evaluate(model, val_loader)
            print(f"Validation - BLEU: {val_metrics['bleu']:.2f}, MCD: {val_metrics['mcd']:.2f}")
```

---

### Phase 3: Debugging & Testing (4-6 hours)

#### Common Issues & Solutions

**Issue 1: Gradient explosion/vanishing**
```python
# Check gradient norms
for name, param in model.named_parameters():
    if param.grad is not None:
        grad_norm = param.grad.norm().item()
        print(f"{name}: {grad_norm:.6f}")
        
# Solution: Gradient clipping (already in training loop)
# If still issues: lower learning rate, check temperature range
```

**Issue 2: NaN losses**
```python
# Common causes:
# 1. Temperature too low (division by near-zero)
# 2. Log of zero in softmax
# 3. Audio loss with mismatched shapes

# Fix: Add epsilon in log operations
log_probs = torch.log(soft_tokens + 1e-10)  # Not + 0

# Fix: Check shapes before computing audio loss
assert generated_audio.shape[0] > 0, "Empty audio generated"
```

**Issue 3: XTTS doesn't accept embeddings**
```python
# You'll need to find where XTTS does:
# x = self.embedding(input_ids)

# Replace with:
# if input is soft embeddings:
#     x = input_embeddings
# else:
#     x = self.embedding(input_ids)

# Look in XTTS source code for the GPT encoder forward pass
```

**Issue 4: Memory issues**
```python
# Joint model is 2x memory
# Solutions:
# 1. Smaller batch size
# 2. Gradient checkpointing
# 3. Mixed precision training (fp16)

from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    outputs = model(batch)
    loss = outputs['total_loss']

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

---

### Phase 4: Evaluation (2-4 hours)

#### Metrics to Track

**Translation Quality:**
- BLEU score (using your existing setup)
- chrF score
- COMET score
- BLASER score

**Audio Quality:**
- MCD (Mel-Cepstral Distortion) - you already have this
- Optionally: synthesize and transcribe back (WER)

**Comparison matrix:**
```
                  BLEU  chrF  COMET  MCD
Baseline (independent)  X     Y     Z    W
Joint training          X'    Y'    Z'   W'

Goal: W' < W (better audio) without X' << X (much worse translation)
```

#### Evaluation Script
```python
def evaluate_joint_model(model, test_loader, metrics):
    model.eval()
    results = {
        'bleu': [],
        'chrf': [],
        'comet': [],
        'mcd': []
    }
    
    with torch.no_grad():
        for batch in test_loader:
            # Generate with discrete tokens (inference mode)
            model.temperature = 0.1  # Low temp for sharp decisions
            outputs = model(batch, use_gumbel=False)  # Discrete inference
            
            # Compute translation metrics
            results['bleu'].append(compute_bleu(outputs['translation'], batch['reference']))
            results['chrf'].append(compute_chrf(outputs['translation'], batch['reference']))
            
            # Compute audio metrics
            results['mcd'].append(compute_mcd(outputs['audio'], batch['reference_audio']))
    
    return {k: np.mean(v) for k, v in results.items()}
```

---

## Data Requirements

### What You Need
```
Each training example:
{
    'source_text': "Hello, how are you?",           # English
    'target_text': "Hola, ¿cómo estás?",           # Your language
    'target_audio': tensor([...]),                  # Audio of target_text
    'speaker_wav': tensor([...])                    # Reference for voice cloning
}
```

### Data Preparation
```python
# Assuming you have:
# - source_texts.txt
# - target_texts.txt  
# - target_audios/ (wav files)

class JointDataset(Dataset):
    def __init__(self, source_file, target_file, audio_dir):
        self.sources = load_texts(source_file)
        self.targets = load_texts(target_file)
        self.audio_paths = sorted(glob(f"{audio_dir}/*.wav"))
        
        assert len(self.sources) == len(self.targets) == len(self.audio_paths)
    
    def __getitem__(self, idx):
        # Load audio
        audio, sr = torchaudio.load(self.audio_paths[idx])
        
        return {
            'source_text': self.sources[idx],
            'target_text': self.targets[idx],
            'target_audio': audio,
            'speaker_wav': audio[:sr * 3]  # First 3 seconds as reference
        }
```

---

## Hyperparameters to Tune

### Critical Parameters
```python
# Temperature schedule
max_temp = 5.0          # Start soft
min_temp = 0.5          # End almost discrete
anneal_steps = 10000    # How long to anneal

# Loss weights
alpha = 0.6             # Translation loss weight
beta = 0.4              # Audio loss weight

# Learning rates
lr_nllb = 1e-5          # Lower for pretrained model
lr_xtts = 1e-6          # Even lower (more stable)

# Training
batch_size = 4          # Small due to memory
grad_clip = 1.0         # Prevent explosions
num_epochs = 5          # Start small
```

### Things to Experiment With
1. **Temperature schedules:** Linear vs exponential vs step-wise
2. **Loss weights:** Fixed vs dynamic vs learned
3. **Freezing strategies:** Freeze XTTS first N epochs, then unfreeze
4. **Audio loss functions:** MSE vs MCD vs perceptual loss

---

## Expected Results & Analysis

### Success Criteria
✅ **Training doesn't diverge** (losses decrease, no NaN)
✅ **Audio quality improves** (MCD decreases)
✅ **Translation quality maintained** (BLEU doesn't drop >5 points)

### What to Analyze
1. **Temperature effect:** Plot translation quality vs temperature
2. **Loss balance:** How do translation_loss and audio_loss evolve?
3. **Examples:** Find cases where joint training chose different translation
4. **Ablations:** What if β=0 (only translation loss)?

### Visualization Ideas
```python
# 1. Loss curves
plt.plot(translation_losses, label='Translation')
plt.plot(audio_losses, label='Audio')
plt.plot(temperatures, label='Temperature')

# 2. Attention patterns
# Compare: What does NLLB attend to when audio-aware vs not?

# 3. Token distribution sharpness
# Plot entropy of soft_tokens over training
entropy = -(soft_tokens * log(soft_tokens)).sum(dim=-1)
```

---

## Troubleshooting Guide

### Problem: Training is too slow
**Solutions:**
- Reduce batch size (memory) or sequence length
- Use mixed precision (fp16)
- Profile to find bottleneck: `torch.profiler`
- Consider freezing XTTS decoder (only train encoder)

### Problem: Translation quality drops significantly
**Solutions:**
- Increase α (translation weight)
- Start with pretrained NLLB frozen, gradually unfreeze
- Use higher min_temperature (stay softer)
- Add KL regularization to keep close to pretrained distribution

### Problem: Audio quality doesn't improve
**Solutions:**
- Check if gradients are flowing (print grad norms)
- Increase β (audio weight)
- Try different audio loss (MCD instead of MSE)
- Verify XTTS is actually updating (check param changes)

### Problem: Gumbel-Softmax returns NaN
**Solutions:**
- Clip temperature (never below 0.1)
- Add epsilon to log operations
- Check for inf/nan in input logits
- Use `torch.autograd.detect_anomaly()`

---

## File Structure
```
project/
├── models/
│   ├── nllb_wrapper.py          # GumbelNLLBDecoder
│   ├── xtts_wrapper.py          # SoftTokenXTTS
│   └── joint_pipeline.py        # JointTranslationTTS
├── training/
│   ├── schedulers.py            # TemperatureScheduler
│   ├── train_joint.py           # Main training loop
│   └── losses.py                # Custom loss functions
├── data/
│   ├── dataset.py               # JointDataset
│   └── preprocess.py            # Data preparation
├── eval/
│   ├── evaluate.py              # Evaluation metrics
│   └── visualize.py             # Plotting utilities
├── configs/
│   └── joint_config.yaml        # Hyperparameters
└── scripts/
    ├── run_training.sh          # Batch job script
    └── debug_gradients.py       # Debugging utilities
```

---

## Quick Reference: Key Functions

### Gumbel-Softmax
```python
def gumbel_softmax(logits, temperature=1.0, hard=False):
    gumbels = -torch.log(-torch.log(torch.rand_like(logits)))
    y = (logits + gumbels) / temperature
    y_soft = F.softmax(y, dim=-1)
    
    if hard:
        # Straight-through: forward hard, backward soft
        y_hard = torch.zeros_like(y_soft)
        y_hard.scatter_(-1, y_soft.argmax(dim=-1, keepdim=True), 1.0)
        y = y_hard - y_soft.detach() + y_soft
    else:
        y = y_soft
    
    return y
```

### Soft Embedding Lookup
```python
def soft_embedding_lookup(soft_tokens, embedding_table):
    # soft_tokens: [batch, seq_len, vocab_size]
    # embedding_table: [vocab_size, hidden_dim]
    return torch.matmul(soft_tokens, embedding_table)
    # output: [batch, seq_len, hidden_dim]
```

### Temperature Update
```python
def update_temperature(step, max_temp, min_temp, total_steps):
    progress = min(1.0, step / total_steps)
    return max_temp - (max_temp - min_temp) * progress
```

---

## Research Questions to Explore

1. **Does joint training improve audio quality without hurting translation?**
   - Measure: BLEU vs MCD trade-off

2. **What temperature schedule works best?**
   - Try: linear, exponential, step-wise

3. **What's the optimal loss weighting (α, β)?**
   - Grid search or dynamic weighting

4. **What linguistic patterns does the model learn to prefer?**
   - Analyze: sentence length, word complexity, syntax in high-quality outputs

5. **Can we visualize what makes translations "TTS-friendly"?**
   - Attention visualization, token choice analysis

---

## Timeline Checklist

### Week 1: Implementation (8 hours)
- [ ] Implement Gumbel-Softmax in NLLB decoder
- [ ] Modify XTTS to accept soft embeddings
- [ ] Create joint model wrapper
- [ ] Test forward pass on small batch
- [ ] Verify gradient flow (no NaN, reasonable magnitudes)

### Week 2: Training (10 hours)
- [ ] Prepare full dataset with audio
- [ ] Set up temperature scheduler
- [ ] Implement training loop with both losses
- [ ] Submit batch job for training
- [ ] Monitor training (losses, gradients, memory)
- [ ] Save checkpoints

### Week 3: Evaluation & Analysis (7 hours)
- [ ] Evaluate on test set (BLEU, chrF, COMET, MCD)
- [ ] Compare to baseline (independent training)
- [ ] Generate example translations + audio
- [ ] Create visualizations (loss curves, temperature effect)
- [ ] Ablation studies (if time)
- [ ] Document findings

---

## Resources & References

### Papers to Read
1. **Gumbel-Softmax:** "Categorical Reparameterization with Gumbel-Softmax" (Jang et al., 2016)
2. **VAE:** "Auto-Encoding Variational Bayes" (Kingma & Welling, 2013)
3. **Straight-Through Estimator:** "Estimating or Propagating Gradients Through Stochastic Neurons" (Bengio et al., 2013)

### Code References
- Gumbel-Softmax implementations: `torch.nn.functional.gumbel_softmax`
- NLLB: Hugging Face Transformers library
- XTTS: Coqui TTS library

### Concepts to Review
- Backpropagation through sampling
- Reparameterization trick
- Temperature in softmax
- Multi-task learning
- Gradient flow visualization

---

## Notes & Observations

### Insights During Implementation
(Add your notes here as you work)

### Unexpected Challenges
(Document problems you encounter)

### Ideas for Future Work
(Things you'd try with more time)

---

**When asking for help, include:**
1. What you're trying to do
2. What you expected
3. What actually happened
4. Relevant code snippet
5. Error messages (if any)